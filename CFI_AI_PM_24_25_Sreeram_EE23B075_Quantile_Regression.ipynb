{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SREERAM-R2005/NIGHT_VISION_SELF/blob/main/CFI_AI_PM_24_25_Sreeram_EE23B075_Quantile_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HGFJ1pQsP8VN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "aVmsGWUFd-tP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "VMJ4av3_YGjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "!pip3 install pyprind\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "hUT0FSfTbvoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instructions\n",
        "\n",
        "* Clone the notebook to your drive.\n",
        "* The notebook has to be submitted in the form of a link giving us **view access**. Share this link in your application.\n",
        "\n",
        "* If you still have any queries, you can reach out to the [core team](https://www.notion.so/Club-Contacts-70a4823e0ae34f35a0aa5d479e449915)\n",
        "\n"
      ],
      "metadata": {
        "id": "q3APxWObeA_c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Common Technical Questionnaire\n"
      ],
      "metadata": {
        "id": "h0ojuzzfedsb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 1\n",
        "\n",
        "Supervised learning is a type of machine learning where the inputs and outputs are mapped through\n",
        "a family of equations, the machine learning model essentially picks the right curve to fit the data.\n",
        "Quantile Regression is a type of supervised learning technique used in statistics and economics. One\n",
        "advantage of quantile regression relative to ordinary least squares regression is that the quantile\n",
        "regression estimates are more robust against outliers in the response measurements.\n",
        "\n",
        "QuantileLossτ (y, ˆy) =\n",
        "{\n",
        "\n",
        "                           τ · (y − ˆy) if y > ˆy\n",
        "\n",
        "                          (1 − τ ) · (ˆy − y) if y ≤ ˆy\n",
        "}\n",
        "\n",
        "where τ is Quantile whose value lies between 0 and 1.\n",
        "Please use this template provided and make changes accordingly for this question alone.\n",
        "Implement a simple Neural Network consisting of 4 nodes, one hidden layer consisting of 5 nodes\n",
        "and output layer consisting of two nodes. Perform quantile regression on the model and observe\n",
        "the loss.\n",
        "**Bonus: Play around with the value of τ to find what value achieves convergence quicker.**\n",
        "\n"
      ],
      "metadata": {
        "id": "IFnXRXAWVSkZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An example implementation of a simple manual neural network is provided. You may use this as inspiration to complete the task at hand."
      ],
      "metadata": {
        "id": "7bKBykeUZtMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n"
      ],
      "metadata": {
        "id": "y5YhkW3jE58O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score={}  # empty dictionary to store the slope of corresponding T values"
      ],
      "metadata": {
        "id": "0-xMziDZ8MnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the Parameters and the Variables\n",
        "# y = a*x + b\n",
        "for i in range(1,20):\n",
        "    torch.manual_seed(423)  # Resetting the seed for each iteration\n",
        "    x = torch.rand((4, 1), requires_grad=True)\n",
        "\n",
        "    a0 = torch.rand((4, 5), requires_grad=True)  # For the hidden layer\n",
        "    b0 = torch.rand((5, 1), requires_grad=True)\n",
        "\n",
        "    a1 = torch.rand((5, 2), requires_grad=True)  # For the output layer. Fill in the dimensions appropriately\n",
        "    b1 = torch.rand((2, 1), requires_grad=True)\n",
        "\n",
        "    # Generating a \"real\" output to compute loss\n",
        "    y = torch.rand((2, 1), requires_grad=False)\n",
        "\n",
        "    quantile = i/20\n",
        "\n",
        "    def loss_function(output, target):\n",
        "        loss = torch.mean(torch.max(quantile * (target - output), (quantile - 1) * (target - output)))\n",
        "        return loss\n",
        "\n",
        "    n_iter=100\n",
        "    losses = []\n",
        "\n",
        "    for i in range(n_iter):\n",
        "\n",
        "        temp1 = a0.T @ x + b0\n",
        "        y_1 = nn.Sigmoid()(temp1)\n",
        "        temp2 = a1.T @ y_1 + b1\n",
        "        y_pred1 = nn.Sigmoid()(temp2)  # Fill in the matrix multiplication equation\n",
        "\n",
        "        loss = loss_function(y_pred1, y)\n",
        "        # print(loss.item())\n",
        "        losses.append(loss.item() * 1e5)\n",
        "\n",
        "        # Back Propagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating Gradients\n",
        "        with torch.no_grad():\n",
        "            a0 -= 0.01 * a0.grad\n",
        "            b0 -= 0.01 * b0.grad\n",
        "            a1 -= 0.01 * a1.grad\n",
        "            b1 -= 0.01 * b1.grad\n",
        "\n",
        "            # Clear gradients\n",
        "            a0.grad = None\n",
        "            b0.grad = None\n",
        "            a1.grad = None\n",
        "            b1.grad = None\n",
        "\n",
        "    slope=losses[1]-losses[0]           #  finding the slope by (y2-y1)/(x2-x1)\n",
        "    score[quantile] = slope      # printing the quantile values and their corresponding slopes(slope of the loss value versus number of iterations)\n"
      ],
      "metadata": {
        "id": "XnVdvfk-Zsyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hU_St3DQ8Zde",
        "outputId": "6a515406-8bce-451c-d242-0e3bbd102db4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0.05: -6.365776062011719,\n",
              " 0.1: -5.713105201721191,\n",
              " 0.15: -5.096197128295898,\n",
              " 0.2: -4.518032073974609,\n",
              " 0.25: -3.966689109802246,\n",
              " 0.3: -3.4570693969726562,\n",
              " 0.35: -2.974271774291992,\n",
              " 0.4: -2.54213809967041,\n",
              " 0.45: -2.1338462829589844,\n",
              " 0.5: -1.761317253112793,\n",
              " 0.55: -1.4275312423706055,\n",
              " 0.6: -1.1265277862548828,\n",
              " 0.65: -0.8642673492431641,\n",
              " 0.7: -0.6340444087982178,\n",
              " 0.75: -0.43958425521850586,\n",
              " 0.8: -0.2808868885040283,\n",
              " 0.85: -0.15869736671447754,\n",
              " 0.9: -0.07003545761108398,\n",
              " 0.95: -0.017136335372924805}"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the Parameters and the Variables\n",
        "# y = a*x + b\n",
        "\n",
        "torch.manual_seed(412)  # SEtting a aprticular seed value so that the rand\n",
        "\n",
        "x = torch.rand((4, 1), requires_grad=True)\n",
        "\n",
        "a0 = torch.rand((4, 5), requires_grad=True)  # For the hidden layer\n",
        "b0 = torch.rand((5, 1), requires_grad=True)\n",
        "\n",
        "a1 = torch.rand((5, 2), requires_grad=True)  # For the output layer. Fill in the dimensions appropriately\n",
        "b1 = torch.rand((2, 1), requires_grad=True)\n",
        "\n",
        "# Generating a \"real\" output to compute loss\n",
        "y = torch.rand((2, 1), requires_grad=False)\n",
        "\n",
        "# Forward Pass 1\n",
        "temp1 = a0.T @ x + b0\n",
        "y_1 = nn.Sigmoid()(temp1)\n",
        "temp2 = a1.T @ y_1 + b1\n",
        "y_pred1 = nn.Sigmoid()(temp2)  # Fill in the matrix multiplication equation\n",
        "\n",
        "quantile = 0.01\n",
        "\n",
        "def loss_function(output, target):\n",
        "    loss = torch.mean(torch.max(quantile * (target - output), (quantile - 1) * (target - output)))\n",
        "    return loss\n",
        "\n",
        "# Computing Loss\n",
        "loss = loss_function(y_pred1, y)\n",
        "print(loss.item())\n",
        "\n",
        "# Back Propagation\n",
        "loss.backward()\n",
        "\n",
        "# Updating Gradients\n",
        "with torch.no_grad():\n",
        "    a0 -= 0.01 * a0.grad\n",
        "    b0 -= 0.01 * b0.grad\n",
        "    a1 -= 0.01 * a1.grad\n",
        "    b1 -= 0.01 * b1.grad\n",
        "\n",
        "    # Clear gradients\n",
        "    a0.grad = None\n",
        "    b0.grad = None\n",
        "    a1.grad = None\n",
        "    b1.grad = None\n",
        "\n",
        "# Forward Pass 2\n",
        "temp1 = a0.T @ x + b0\n",
        "y_1 = nn.Sigmoid()(temp1)\n",
        "temp2 = a1.T @ y_1 + b1\n",
        "y_pred2 = nn.Sigmoid()(temp2)  # Fill in the matrix multiplication equation\n",
        "\n",
        "# Computing Loss\n",
        "loss = loss_function(y_pred2, y)\n",
        "print(loss.item())\n",
        "\n",
        "# Back Propagation\n",
        "loss.backward()\n",
        "\n",
        "# Updating Gradients\n",
        "with torch.no_grad():\n",
        "    a0 -= 0.01 * a0.grad\n",
        "    b0 -= 0.01 * b0.grad\n",
        "    a1 -= 0.01 * a1.grad\n",
        "    b1 -= 0.01 * b1.grad\n",
        "\n",
        "    # Clear gradients\n",
        "    a0.grad = None\n",
        "    b0.grad = None\n",
        "    a1.grad = None\n",
        "    b1.grad = None\n",
        "\n",
        "# Forward Pass 3\n",
        "temp1 = a0.T @ x + b0\n",
        "y_1 = nn.Sigmoid()(temp1)\n",
        "temp2 = a1.T @ y_1 + b1\n",
        "y_pred3 = nn.Sigmoid()(temp2)  # Fill in the matrix multiplication equation\n",
        "\n",
        "# Computing Loss\n",
        "loss = loss_function(y_pred3, y)\n",
        "print(loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73054d48-be54-4904-c785-ac6a755299d9",
        "id": "MFA0kpr9qt6V"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7370193004608154\n",
            "0.7369542717933655\n",
            "0.7368890643119812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ovBJyRBAJEOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "izhl-9Wta_uu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lug2-6fFrBLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Initializing the Parameters and the Variables\n",
        "# y = a*x + b\n",
        "\n",
        "x = torch.rand((4, 1), requires_grad=True)\n",
        "\n",
        "a0 = torch.rand((4, 5), requires_grad=True)  # For the hidden layer\n",
        "b0 = torch.rand((5, 1), requires_grad=True)\n",
        "\n",
        "a1 = torch.rand((5, 2), requires_grad=True)  # For the output layer. Fill in the dimensions appropriately\n",
        "b1 = torch.rand((2, 1), requires_grad=True)\n",
        "\n",
        "# Generating a \"real\" output to compute loss\n",
        "y = torch.rand((2, 1), requires_grad=False)\n",
        "\n",
        "# Forward Pass 1\n",
        "temp1 = a0.T @ x + b0\n",
        "y_1 = nn.Sigmoid()(temp1)\n",
        "temp2 = a1.T @ y_1 + b1\n",
        "y_pred1 = nn.Sigmoid()(temp2)  # Fill in the matrix multiplication equation\n",
        "\n",
        "quantile = 0.3\n",
        "\n",
        "def loss_function(output, target):\n",
        "    loss = torch.mean(torch.max(quantile * (target - output), (quantile - 1) * (target - output)))\n",
        "    return loss\n",
        "\n",
        "# Computing Loss\n",
        "loss = loss_function(y_pred1, y)\n",
        "print(loss.item())\n",
        "\n",
        "# Back Propagation\n",
        "loss.backward()\n",
        "\n",
        "# Updating Gradients\n",
        "with torch.no_grad():\n",
        "    a0 -= 0.01 * a0.grad\n",
        "    b0 -= 0.01 * b0.grad\n",
        "    a1 -= 0.01 * a1.grad\n",
        "    b1 -= 0.01 * b1.grad\n",
        "\n",
        "    # Clear gradients\n",
        "    a0.grad = None\n",
        "    b0.grad = None\n",
        "    a1.grad = None\n",
        "    b1.grad = None\n",
        "\n",
        "# Forward Pass 2\n",
        "temp1 = a0.T @ x + b0\n",
        "y_1 = nn.Sigmoid()(temp1)\n",
        "temp2 = a1.T @ y_1 + b1\n",
        "y_pred2 = nn.Sigmoid()(temp2)  # Fill in the matrix multiplication equation\n",
        "\n",
        "# Computing Loss\n",
        "loss = loss_function(y_pred2, y)\n",
        "print(loss.item())\n",
        "\n",
        "# Back Propagation\n",
        "loss.backward()\n",
        "\n",
        "# Updating Gradients\n",
        "with torch.no_grad():\n",
        "    a0 -= 0.01 * a0.grad\n",
        "    b0 -= 0.01 * b0.grad\n",
        "    a1 -= 0.01 * a1.grad\n",
        "    b1 -= 0.01 * b1.grad\n",
        "\n",
        "    # Clear gradients\n",
        "    a0.grad = None\n",
        "    b0.grad = None\n",
        "    a1.grad = None\n",
        "    b1.grad = None\n",
        "\n",
        "# Forward Pass 3\n",
        "temp1 = a0.T @ x + b0\n",
        "y_1 = nn.Sigmoid()(temp1)\n",
        "temp2 = a1.T @ y_1 + b1\n",
        "y_pred3 = nn.Sigmoid()(temp2)  # Fill in the matrix multiplication equation\n",
        "\n",
        "# Computing Loss\n",
        "loss = loss_function(y_pred3, y)\n",
        "print(loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9f237ca-173e-49ba-a16a-04f2c93bab51",
        "id": "1SlzIIZurDMd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3018702268600464\n",
            "0.30179736018180847\n",
            "0.301724374294281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "0.30179736018180847-0.301724374294281"
      ],
      "metadata": {
        "id": "ZyUtFZFcrJZ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac4ee546-d911-4af4-c9e2-1b8cd4e4ba18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.298588752746582e-05"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Initializing the Parameters and the Variables\n",
        "# y = a*x + b\n",
        "\n",
        "x = torch.rand((4, 1), requires_grad=True)\n",
        "\n",
        "a0 = torch.rand((4, 5), requires_grad=True)  # For the hidden layer\n",
        "b0 = torch.rand((5, 1), requires_grad=True)\n",
        "\n",
        "a1 = torch.rand((5, 2), requires_grad=True)  # For the output layer. Fill in the dimensions appropriately\n",
        "b1 = torch.rand((2, 1), requires_grad=True)\n",
        "\n",
        "# Generating a \"real\" output to compute loss\n",
        "y = torch.rand((2, 1), requires_grad=False)\n",
        "\n",
        "# Forward Pass 1\n",
        "temp1 = a0.T @ x + b0\n",
        "y_1 = nn.Sigmoid()(temp1)\n",
        "temp2 = a1.T @ y_1 + b1\n",
        "y_pred1 = nn.Sigmoid()(temp2)  # Fill in the matrix multiplication equation\n",
        "\n",
        "quantile = 0.1\n",
        "\n",
        "def loss_function(output, target):\n",
        "    loss = torch.mean(torch.max(quantile * (target - output), (quantile - 1) * (target - output)))\n",
        "    return loss\n",
        "\n",
        "# Computing Loss\n",
        "loss = loss_function(y_pred1, y)\n",
        "print(loss.item())\n",
        "\n",
        "# Back Propagation\n",
        "loss.backward()\n",
        "\n",
        "# Updating Gradients\n",
        "with torch.no_grad():\n",
        "    a0 -= 0.01 * a0.grad\n",
        "    b0 -= 0.01 * b0.grad\n",
        "    a1 -= 0.01 * a1.grad\n",
        "    b1 -= 0.01 * b1.grad\n",
        "\n",
        "    # Clear gradients\n",
        "    a0.grad = None\n",
        "    b0.grad = None\n",
        "    a1.grad = None\n",
        "    b1.grad = None\n",
        "\n",
        "# Forward Pass 2\n",
        "temp1 = a0.T @ x + b0\n",
        "y_1 = nn.Sigmoid()(temp1)\n",
        "temp2 = a1.T @ y_1 + b1\n",
        "y_pred2 = nn.Sigmoid()(temp2)  # Fill in the matrix multiplication equation\n",
        "\n",
        "# Computing Loss\n",
        "loss = loss_function(y_pred2, y)\n",
        "print(loss.item())\n",
        "\n",
        "# Back Propagation\n",
        "loss.backward()\n",
        "\n",
        "# Updating Gradients\n",
        "with torch.no_grad():\n",
        "    a0 -= 0.01 * a0.grad\n",
        "    b0 -= 0.01 * b0.grad\n",
        "    a1 -= 0.01 * a1.grad\n",
        "    b1 -= 0.01 * b1.grad\n",
        "\n",
        "    # Clear gradients\n",
        "    a0.grad = None\n",
        "    b0.grad = None\n",
        "    a1.grad = None\n",
        "    b1.grad = None\n",
        "\n",
        "# Forward Pass 3\n",
        "temp1 = a0.T @ x + b0\n",
        "y_1 = nn.Sigmoid()(temp1)\n",
        "temp2 = a1.T @ y_1 + b1\n",
        "y_pred3 = nn.Sigmoid()(temp2)  # Fill in the matrix multiplication equation\n",
        "\n",
        "# Computing Loss\n",
        "loss = loss_function(y_pred3, y)\n",
        "print(loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60730c87-122f-4539-a3bd-8b814704ae1f",
        "id": "sNVvmLqSrJnV"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.40667852759361267\n",
            "0.4065755009651184\n",
            "0.40647223591804504\n"
          ]
        }
      ]
    }
  ]
}